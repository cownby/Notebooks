{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out Spark 2.0\n",
    "\n",
    "aka the *Decimal Project*\n",
    "\n",
    "### Notes/Discoveries\n",
    "\n",
    "* dates are a real headache\n",
    "\n",
    "### References / REad\n",
    "\n",
    "* http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "* https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html\n",
    "* http://blog.cloudera.com/blog/2015/07/how-to-do-data-quality-checks-using-apache-spark-dataframes/\n",
    "* https://www.infoq.com/articles/apache-spark-sql\n",
    "* http://blog.brakmic.com/data-science-for-losers-part-5-spark-dataframes/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CELL A #####\n",
    "\n",
    "# Basic setup required in all notebooks\n",
    " \n",
    "appName='archetest'\n",
    "master='local[*]' #local spark-master\n",
    "\n",
    "# Explicitly define python 2 since we have both 2 & 3 installed\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'\n",
    "\n",
    "# Establish spark context or session\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "#conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# A SparkSession can be created using a builder pattern\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(master)\n",
    "         .appName(appName)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> DataFrame[id: bigint]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL A-2 #####\n",
    "\n",
    "# do something to prove it works\n",
    "\n",
    "# Old way\n",
    "#rdd = sc.parallelize(range(1000))\n",
    "#rdd.takeSample(False, 5)\n",
    "\n",
    "# Spark 2\n",
    "# ok- rdd's are still available, but the spark dataframe/set is now the preferred data structure, \n",
    "# albeit a bit more complex.  For example, you don't create a dataframe or rdd from a random  \n",
    "# list, you create a range that is ALREADY a dataframe.\n",
    "\n",
    "r = spark.range(10) #creates a dataframe/dataset\n",
    "print type(r),r\n",
    "r.sample(False,.5)\n",
    "r.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Somewhat Handy methods\n",
    "\n",
    "def prSomeDS(inputDS,howMany,msg=None):\n",
    "    \"\"\"\n",
    "    Debugging shorthand\n",
    "\n",
    "    Args: A Spark 2 DataSet, how many items to print, and optional message\n",
    "\n",
    "    Returns: None\n",
    "        \n",
    "    \"\"\"\n",
    "    print msg\n",
    "    #inputDS.take(howMany).foreach(println)  #doesn't work with all spark2 datasets\n",
    "    for x in inputDS.take(howMany):\n",
    "        print x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop here  # halt execution with an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## File Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value=u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band'),\n",
       " Row(value=u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL B #####\n",
    "# read local file into a dataset\n",
    "\n",
    "datafile_local = \"/home/jovyan/work/fotd.csv\"\n",
    "\n",
    "#dataRDD = sc.textFile(datafile_local)  #old way\n",
    "\n",
    "dataDS = spark.read.text(datafile_local) # spark2\n",
    "#spark2 type is <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "print type(dataDS)\n",
    "dataDS.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "fotd sample from hdfs store\n",
      "Row(value=u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band')\n",
      "Row(value=u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam')\n"
     ]
    }
   ],
   "source": [
    "##### CELL C #####\n",
    "\n",
    "# Define the datafile and establish HDFS access, spark 2\n",
    "\n",
    "datafile_HDFS = r'hdfs://172.18.0.2:9000/user/root/testData/fotd.csv'\n",
    "d = spark.read.text(datafile_HDFS)\n",
    "print type(d)\n",
    "prSomeDS(d,2,'fotd sample from hdfs store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- sheet: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Lesson: string (nullable = true)\n",
      " |-- attendees_reg: string (nullable = true)\n",
      " |-- attendees_student: string (nullable = true)\n",
      " |-- attendees_special: string (nullable = true)\n",
      " |-- attendees_board: string (nullable = true)\n",
      " |-- addendees_free: string (nullable = true)\n",
      " |-- addendees_coupon: string (nullable = true)\n",
      " |-- addendees_comp: string (nullable = true)\n",
      " |-- fee_reg: string (nullable = true)\n",
      " |-- fee_student: string (nullable = true)\n",
      " |-- fee_special: string (nullable = true)\n",
      " |-- fee_board: string (nullable = true)\n",
      " |-- donations: string (nullable = true)\n",
      " |-- caller: string (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read as default csv file\n",
    "d = spark.read.csv(datafile_HDFS, header = \"true\")\n",
    "print type(d)\n",
    "d.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion\n",
    "Get Data into a useable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Cell Data Conversion-A #####\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace, trim, col, lower, to_date\n",
    "\n",
    "# extraction patterns\n",
    "re_us_currency = r'(\\d*\\.\\d\\d)'\n",
    "re_integer = r'(\\d*)'\n",
    "re_float = r'(\\d*\\.?\\d*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newdate: date (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- caller: string (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      " |-- inlesson: integer (nullable = true)\n",
      " |-- attendees_reg: integer (nullable = true)\n",
      " |-- attendees_student: integer (nullable = true)\n",
      " |-- attendees_special: integer (nullable = true)\n",
      " |-- attendees_board: integer (nullable = true)\n",
      " |-- addendees_coupon: integer (nullable = true)\n",
      " |-- addendees_coupon: integer (nullable = true)\n",
      " |-- fee_reg: decimal(10,2) (nullable = true)\n",
      " |-- fee_student: decimal(10,2) (nullable = true)\n",
      " |-- fee_special: decimal(10,2) (nullable = true)\n",
      " |-- fee_board: decimal(10,2) (nullable = true)\n",
      " |-- donations: decimal(10,2) (nullable = true)\n",
      "\n",
      "+-------+--------------------+---------------+--------------------+--------+-------------+-----------------+-----------------+---------------+----------------+----------------+-------+-----------+-----------+---------+---------+\n",
      "|newdate|               event|         caller|                band|inlesson|attendees_reg|attendees_student|attendees_special|attendees_board|addendees_coupon|addendees_coupon|fee_reg|fee_student|fee_special|fee_board|donations|\n",
      "+-------+--------------------+---------------+--------------------+--------+-------------+-----------------+-----------------+---------------+----------------+----------------+-------+-----------+-----------+---------+---------+\n",
      "|   null|contra dance fina...|patricia dancen|               steam|    null|           37|               11|                0|              4|               0|               0|   9.00|       5.00|       8.00|     9.00|    37.00|\n",
      "|   null|contra dance fina...|    duffy boyle|     balance & swing|       0|           41|                6|                0|              3|               2|               2|   9.00|       5.00|       8.00|     9.00|    61.00|\n",
      "|   null|contra dance fina...|      sam smith|john reading/jose...|       0|           56|                6|                1|              4|               5|               5|   9.00|       5.00|       8.00|     9.00|    78.00|\n",
      "+-------+--------------------+---------------+--------------------+--------+-------------+-----------------+-----------------+---------------+----------------+----------------+-------+-----------+-----------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Cell Data Conversion-B #####\n",
    "\n",
    "# Transform into new dataset/dataframe with appropriate data types\n",
    "ds = dataDS.select(to_date(regexp_replace(d.Date,r'/',r'-')).alias('newdate'),\n",
    "                   lower(d.Event).alias('event'),\n",
    "                   lower(d.caller).alias('caller'),\n",
    "                   lower(d.band).alias('band'),\n",
    "                   regexp_extract(d.Lesson,re_integer,1).alias('inlesson').cast(\"int\"), \n",
    "                   regexp_extract(d.attendees_reg,re_integer,1).alias('attendees_reg').cast(\"int\"), \n",
    "                   regexp_extract(d.attendees_student,re_integer,1).alias('attendees_student').cast(\"int\"), \n",
    "                   regexp_extract(d.attendees_special,re_integer,1).alias('attendees_special').cast(\"int\"), \n",
    "                   regexp_extract(d.attendees_board,re_integer,1).alias('attendees_board').cast(\"int\"), \n",
    "                   regexp_extract(d.addendees_coupon,re_integer,1).alias('addendees_coupon').cast(\"int\"), \n",
    "                   regexp_extract(d.addendees_coupon,re_integer,1).alias('addendees_coupon').cast(\"int\"), \n",
    "                   regexp_extract(d.fee_reg,re_us_currency,1).alias(\"fee_reg\").cast(\"decimal(10,2)\"),\n",
    "                   regexp_extract(d.fee_student,re_us_currency,1).alias(\"fee_student\").cast(\"decimal(10,2)\"),\n",
    "                   regexp_extract(d.fee_special,re_us_currency,1).alias(\"fee_special\").cast(\"decimal(10,2)\"),\n",
    "                   regexp_extract(d.fee_board,re_us_currency,1).alias(\"fee_board\").cast(\"decimal(10,2)\"),\n",
    "                   regexp_extract(d.donations,re_us_currency,1).alias(\"donations\").cast(\"decimal(10,2)\")\n",
    "                  )\n",
    "              \n",
    "ds.printSchema()\n",
    "#prSomeDS(ds,3,\"transformed data\")\n",
    "ds.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#once I have the data, do something with it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_conf',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedContext',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_jwrapped',\n",
       " '_sc',\n",
       " '_wrapped',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what CAN I do with this spark session object?\n",
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
