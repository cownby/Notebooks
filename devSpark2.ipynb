{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out Spark 2.0\n",
    "\n",
    "aka the *Decimal Project*\n",
    "\n",
    "### Notes/Discoveries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CELL A #####\n",
    "\n",
    "# Basic setup required in all notebooks\n",
    " \n",
    "appName='archetest'\n",
    "master='local[*]' #local spark-master\n",
    "\n",
    "# Explicitly define python 2 since we have both 2 & 3 installed\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'\n",
    "\n",
    "# Establish spark context or session\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# A SparkSession can be created using a builder pattern\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(master)\n",
    "         .appName(appName)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> DataFrame[id: bigint]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do something to prove it works\n",
    "\n",
    "# Old way\n",
    "#rdd = sc.parallelize(range(1000))\n",
    "#rdd.takeSample(False, 5)\n",
    "\n",
    "# Spark 2\n",
    "# ok- rdd's are still available, but the spark dataframe/set is now the preferred data structure, \n",
    "# albeit a bit more complex.  For example, you don't create a dataframe or rdd from a random  \n",
    "# list, you create a range that is ALREADY a dataframe.\n",
    "r = spark.range(10) #creates a dataframe/dataset\n",
    "print type(r),r\n",
    "r.sample(False,.5)\n",
    "r.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handy methods\n",
    "\n",
    "def prSomeDS(inputDS,howMany,msg=None):\n",
    "    \"\"\"\n",
    "    Debugging shorthand\n",
    "\n",
    "    Args: A Spark 2 DataSet, how many items to print, and optional message\n",
    "\n",
    "    Returns: None\n",
    "        \n",
    "    \"\"\"\n",
    "    print msg\n",
    "    #inputDS.take(howMany).foreach(println)\n",
    "    for x in inputDS.take(howMany):\n",
    "        print x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop here  # halt execution with an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## File Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value=u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band'),\n",
       " Row(value=u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL B #####\n",
    "\n",
    "# read local file into RDD\n",
    "datafile_local = \"/home/jovyan/work/fotd.csv\"\n",
    "#d = sc.textFile(datafile_local)  #old way\n",
    "d = spark.read.text(datafile_local) # spark2\n",
    "#spark2 type is <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "print type(d)\n",
    "d.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "fotd sample from hdfs store\n",
      "Row(value=u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band')\n",
      "Row(value=u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam')\n"
     ]
    }
   ],
   "source": [
    "##### CELL C #####\n",
    "\n",
    "# Define the datafile and establish HDFS access, spark 2\n",
    "\n",
    "datafile_HDFS = r'hdfs://172.18.0.2:9000/user/root/testData/fotd.csv'\n",
    "d = spark.read.text(datafile_HDFS)\n",
    "print type(d)\n",
    "prSomeDS(d,2,'fotd sample from hdfs store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- sheet: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Lesson: string (nullable = true)\n",
      " |-- attendees_reg: string (nullable = true)\n",
      " |-- attendees_student: string (nullable = true)\n",
      " |-- attendees_special: string (nullable = true)\n",
      " |-- attendees_board: string (nullable = true)\n",
      " |-- addendees_free: string (nullable = true)\n",
      " |-- addendees_coupon: string (nullable = true)\n",
      " |-- addendees_comp: string (nullable = true)\n",
      " |-- fee_reg: string (nullable = true)\n",
      " |-- fee_student: string (nullable = true)\n",
      " |-- fee_special: string (nullable = true)\n",
      " |-- fee_board: string (nullable = true)\n",
      " |-- donations: string (nullable = true)\n",
      " |-- caller: string (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = spark.read.csv(datafile_HDFS, header = \"true\")\n",
    "print type(d)\n",
    "d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def returnCurrency(column):\n",
    "    \"\"\"Returns only legal us currency values from input column\n",
    "\n",
    "    Note:\n",
    "\n",
    "    Args:\n",
    "        column (Column): A Column containing a string.\n",
    "\n",
    "    Returns:\n",
    "        Column: A Column \n",
    "    \"\"\"\n",
    "\n",
    "    return (regexp_replace(column, '^([0-9]*\\.[0-9][0-9])', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sheet: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- attendees_reg: double (nullable = true)\n",
      " |-- nip: string (nullable = true)\n",
      "\n",
      "transformed data\n",
      "Row(sheet=u'4/16/2016', Date=u'4/16/2016', attendees_reg=37.0, nip=u'$9.00')\n",
      "Row(sheet=u'4/2/2016', Date=u'4/2/2016', attendees_reg=41.0, nip=u'$9.00')\n",
      "Row(sheet=u'3/19/2016', Date=u'3/19/2016', attendees_reg=56.0, nip=u'$9.00')\n",
      "original data\n",
      "Row(sheet=u'4/16/2016', Date=u'4/16/2016', Event=u'Contra Dance Financial Worksheet', Lesson=u'-1', attendees_reg=u'37', attendees_student=u'11', attendees_special=u'0', attendees_board=u'4', addendees_free=u'6', addendees_coupon=u'0', addendees_comp=u'6', fee_reg=u'$9.00', fee_student=u'$5.00', fee_special=u'$8.00', fee_board=u'$9.00', donations=u'$37.00', caller=u'Patricia Dancen', band=u'Steam')\n",
      "Row(sheet=u'4/2/2016', Date=u'4/2/2016', Event=u'Contra Dance Financial Worksheet', Lesson=u'0', attendees_reg=u'41', attendees_student=u'6', attendees_special=u'0', attendees_board=u'3', addendees_free=u'2', addendees_coupon=u'2', addendees_comp=u'5', fee_reg=u'$9.00', fee_student=u'$5.00', fee_special=u'$8.00', fee_board=u'$9.00', donations=u'$61.00', caller=u'Duffy Boyle', band=u'Balance & Swing')\n",
      "Row(sheet=u'3/19/2016', Date=u'3/19/2016', Event=u'Contra Dance Financial Worksheet', Lesson=u'0', attendees_reg=u'56', attendees_student=u'6', attendees_special=u'1', attendees_board=u'4', addendees_free=u'0', addendees_coupon=u'5', addendees_comp=u'5', fee_reg=u'$9.00', fee_student=u'$5.00', fee_special=u'$8.00', fee_board=u'$9.00', donations=u'$78.00', caller=u'Sam Smith', band=u'John Reading/Josey Toney')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = d.select(d.sheet, d.Date, d.attendees_reg.cast(\"double\"), returnCurrency(d.fee_reg).alias(\"nip\")) #cast(\"decimal(10,2)\"))\n",
    "ds.printSchema()\n",
    "prSomeDS(ds,3,\"transformed data\")\n",
    "prSomeDS(d,3,\"original data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#once I have the data, do something with it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
