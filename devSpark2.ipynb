{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out Spark 2.0\n",
    "\n",
    "aka the *Decimal Project*\n",
    "\n",
    "### Notes/Discoveries\n",
    "an empty core-site.xml causes \n",
    "```\n",
    "---> 18 sc = SparkContext(conf=conf)\n",
    "Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
    " : java.lang.ExceptionInInitializerError\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[432, 207, 356, 638, 47]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL A #####\n",
    "\n",
    "# Basic setup required in all notebooks\n",
    " \n",
    "appName='archetest'\n",
    "master='local[*]' #local spark-master\n",
    "\n",
    "# Explicitly define python 2 since we have both 2 & 3 installed\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'\n",
    "\n",
    "# Establish spark context or session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band',\n",
       " u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL B #####\n",
    "\n",
    "# read local file\n",
    "localFilePath = \"/home/jovyan/work/fotd.csv\"\n",
    "d = sc.textFile(localFilePath)\n",
    "print type(d)\n",
    "d.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### CELL C #####\n",
    "\n",
    "#import hadoop\n",
    "def set_hadoop_config():\n",
    "    \n",
    "    # Turns out setting parameters via hadoopConfiguration is (apparently) not useful for spark 2.0\n",
    "    \n",
    "    #Required parameter names listed in http://spark.apache.org/docs/latest/storage-openstack-swift.html\n",
    "\n",
    "    prefix = \"fs.hdfs.service.\" + appName  #some refs use creds['name'], which can be any string\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + \".auth.url\", r'http://172.18.0.2' )\n",
    "    hconf.set(prefix + \".username\", 'root')\n",
    "    hconf.set(prefix + \".http.port\", '9000')\n",
    "    hconf.setBoolean(prefix + \".public\", True)\n",
    "\n",
    "    # TRY: download jars & add to Dockerfile\n",
    "    #org.apache.hadoop.\n",
    "    #hconf.set(\"fs.hdfs.impl\", hdfs.DistributedFileSystem.getName());\n",
    "    #hconf.set(\"fs.file.impl\", org.apache.hadoop.fs.LocalFileSystem.getName());\n",
    "    #hconf.set(\"fs.swift.impl\", \"org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'sheet,Date,Event,Lesson,attendees_reg,attendees_student,attendees_special,attendees_board,addendees_free,addendees_coupon,addendees_comp,fee_reg,fee_student,fee_special,fee_board,donations,caller,band',\n",
       " u'4/16/2016,4/16/2016,Contra Dance Financial Worksheet,-1,37,11,0,4,6,0,6,$9.00,$5.00,$8.00,$9.00,$37.00,Patricia Dancen,Steam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### CELL D #####\n",
    "\n",
    "# Define the datafile and establish access, spark 2\n",
    "\n",
    "#set_hadoop_config()  # not helpful!\n",
    "\n",
    "datafileHDFS = r'hdfs://172.18.0.2:9000/user/root/testData/fotd.csv'\n",
    "d = sc.textFile(datafileHDFS)\n",
    "print type(d)\n",
    "d.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-b71a775f3f87>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-b71a775f3f87>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here  #halt execution with an error\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here  # halt execution with an error\n",
    "\n",
    "\n",
    "#this is stuff I might yet use\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APACHE_SPARK_VERSION': '2.0.0',\n",
       " 'CLICOLOR': '1',\n",
       " 'CONDA_DIR': '/opt/conda',\n",
       " 'DEBIAN_FRONTEND': 'noninteractive',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'GRANT_SUDO': 'yes',\n",
       " 'HOME': '/home/jovyan',\n",
       " 'HOSTNAME': '93ad4a639d79',\n",
       " 'JPY_PARENT_PID': '9',\n",
       " 'LANG': 'en_US.UTF-8',\n",
       " 'LANGUAGE': 'en_US.UTF-8',\n",
       " 'LC_ALL': 'en_US.UTF-8',\n",
       " 'LOGNAME': 'jovyan',\n",
       " 'MAIL': '/var/mail/jovyan',\n",
       " 'MESOS_NATIVE_LIBRARY': '/usr/local/lib/libmesos.so',\n",
       " 'NB_UID': '1000',\n",
       " 'NB_USER': 'jovyan',\n",
       " 'PAGER': 'cat',\n",
       " 'PATH': '/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       " 'PWD': '/home/jovyan/work',\n",
       " 'PYSPARK_PYTHON': '/opt/conda/envs/python2/bin/python',\n",
       " 'PYTHONPATH': '/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.1-src.zip',\n",
       " 'R_LIBS_USER': '/usr/local/spark/R/lib',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'SPARK_HOME': '/usr/local/spark',\n",
       " 'SPARK_OPTS': '--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info',\n",
       " 'TERM': 'xterm-color',\n",
       " 'USER': 'jovyan',\n",
       " '_': '/usr/bin/env'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#once I have the data, do something with it\n",
    "\n",
    "dataRDD2 = dataRDD.map(lambda x: x.split('\\r\\n'))\n",
    "print type(dataRDD2), dataRDD2\n",
    "print dataRDD2.count() \n",
    "prSomeRDD(dataRDD2,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handy methods\n",
    "\n",
    "\n",
    "def prSomeRDD(inputRDD,howMany,msg=None):\n",
    "    \"\"\"\n",
    "    Debugging shorthand\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns: None\n",
    "        \n",
    "    \"\"\"\n",
    "    print msg\n",
    "    #inputRDD.take(howMany).foreach(println)\n",
    "    for x in inputRDD.take(howMany):\n",
    "        print x    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
