{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out Spark 2.0\n",
    "\n",
    "aka the *Decimal Project*\n",
    "\n",
    "### Notes/Discoveries\n",
    "\n",
    "* dates are a real headache\n",
    "* rdd's are still available, but the spark dataframe/set is now the preferred data structure, \n",
    "albeit a bit more complex.  For example, you don't create a dataframe or rdd from a random  \n",
    "list, you create a range that is ALREADY a dataframe.\n",
    "\n",
    "\n",
    "### References / Read\n",
    "\n",
    "* http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "* https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html\n",
    "* http://blog.cloudera.com/blog/2015/07/how-to-do-data-quality-checks-using-apache-spark-dataframes/\n",
    "* https://www.infoq.com/articles/apache-spark-sql\n",
    "* http://blog.brakmic.com/data-science-for-losers-part-5-spark-dataframes/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CELL A #####\n",
    "# Basic setup required in all notebooks\n",
    " \n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "appName='archetest'\n",
    "master='local[*]' #local spark-master\n",
    "\n",
    "# Explicitly define python 2 since we have both 2 & 3 installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(master)\n",
    "         .appName(appName)\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[801, 171, 954, 867, 441]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'> [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n"
     ]
    }
   ],
   "source": [
    "##### CELL A-2 #####\n",
    "# verify it works\n",
    "\n",
    "# Old way\n",
    "rdd = spark.sparkContext.parallelize(range(1000))\n",
    "print rdd.takeSample(False, 5)\n",
    "\n",
    "# Spark 2\n",
    "# rdd's are still available, but the spark dataframe/set is now the preferred data structure, \n",
    "# albeit a bit more complex.  For example, you don't create a dataframe or rdd from a random  \n",
    "# list, you create a range that is ALREADY a dataframe.\n",
    "r = spark.range(10) #creates a dataframe/dataset\n",
    "print type(r) ,r.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop here  # halt execution with an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Parse a Clean Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### File Access\n",
    "\n",
    "Read as text from local and HDFS stores. Use Spark 2.0 native csv functionality to read CSV & derive schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(value=u'lineNum,aStr,aDate,anInt,aFloat,aDollarAmount,aNote'), Row(value=u'1,smart,1/1/2016,1,1.1, $1.10 ,good data')]\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL READ A #####\n",
    "# read local file into a dataset\n",
    "\n",
    "datafile_local = \"/home/jovyan/work/data/simpleSampleClean.csv\"\n",
    "\n",
    "#raw_dataRDD = sc.textFile(datafile_local)  # spark 1.6\n",
    "raw_data1 = spark.read.text(datafile_local) # spark 2.0\n",
    "\n",
    "print type(raw_data1)   #spark2 type is <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "print raw_data1.take(2)\n",
    "raw_data1.printSchema()\n",
    "#raw_data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(value=u'lineNum,aStr,aDate,anInt,aFloat,aDollarAmount,aNote'), Row(value=u'1,smart,2016/01/01,1,1.1, $1.10 ,good data')]\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL READ B #####\n",
    "# Read file from HDFS\n",
    "\n",
    "datafile_HDFS = r'hdfs://172.18.0.2:9000/user/root/testData/simpleSampleClean.csv'\n",
    "\n",
    "raw_data2 = spark.read.text(datafile_HDFS)\n",
    "\n",
    "print type(raw_data2)\n",
    "print raw_data2.take(2)\n",
    "raw_data2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- lineNum: string (nullable = true)\n",
      " |-- aStr: string (nullable = true)\n",
      " |-- aDate: string (nullable = true)\n",
      " |-- anInt: string (nullable = true)\n",
      " |-- aFloat: string (nullable = true)\n",
      " |-- aDollarAmount: string (nullable = true)\n",
      " |-- aNote: string (nullable = true)\n",
      "\n",
      "+-------+------+----------+-----+------+-------------+---------+\n",
      "|lineNum|  aStr|     aDate|anInt|aFloat|aDollarAmount|    aNote|\n",
      "+-------+------+----------+-----+------+-------------+---------+\n",
      "|      1| smart|2016/01/01|    1|   1.1|       $1.10 |good data|\n",
      "|      2|  cows|  2016/1/2|    2|   2.2|       $2.20 |good data|\n",
      "|      3|   moo|  2016/1/3|    3|   3.3|       $3.30 |good data|\n",
      "|      4|longer|  2016/1/4|    4|   4.4|       $4.40 |good data|\n",
      "+-------+------+----------+-----+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL READ  C #####\n",
    "# Read as default csv file with inferred header\n",
    "\n",
    "raw_data2 = spark.read.csv(datafile_HDFS, header = \"true\")\n",
    "print type(raw_data2)   # still a dataframe\n",
    "raw_data2.printSchema()\n",
    "raw_data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- line: string (nullable = true)\n",
      " |-- aStr: string (nullable = true)\n",
      " |-- aDate: date (nullable = true)\n",
      " |-- anInt: integer (nullable = true)\n",
      " |-- aFloat: double (nullable = true)\n",
      " |-- aDollarAmount: string (nullable = true)\n",
      " |-- aNote: string (nullable = true)\n",
      "\n",
      "+----+----+-----+-----+------+-------------+-----+\n",
      "|line|aStr|aDate|anInt|aFloat|aDollarAmount|aNote|\n",
      "+----+----+-----+-----+------+-------------+-----+\n",
      "+----+----+-----+-----+------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL READ  D #####\n",
    "# Open a simple file with pre-defined schema\n",
    "# Drop for now; dates aren't worth the hassle\n",
    "\n",
    "datafile_local = \"/home/jovyan/work/data/simpleSampleClean_v2.csv\"\n",
    "\n",
    "schema = StructType([StructField(\"line\", StringType(), False),\n",
    "                     StructField(\"aStr\", StringType(), False),\n",
    "                     #StructField(\"aDate\", StringType(), False), #\n",
    "                     StructField(\"aDate\", DateType(), False),\n",
    "                     StructField(\"anInt\", IntegerType(), False),\n",
    "                     StructField(\"aFloat\", DoubleType(), False),\n",
    "                     StructField(\"aDollarAmount\",StringType(),True),\n",
    "                     StructField(\"aNote\", StringType(), False)\n",
    "                     ])  \n",
    "raw_data3 = spark.read.csv(datafile_local,\n",
    "                             schema = schema,\n",
    "                             mode = 'DROPMALFORMED',  # PERMISSIVE DROPMALFORMED FAILFAST\n",
    "                             sep = ',',\n",
    "                             comment='#'\n",
    "                            )\n",
    "\n",
    "print type(raw_data3)\n",
    "raw_data3.printSchema()\n",
    "raw_data3.show()\n",
    "#raw_data3.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion (clean file)\n",
    "Get Data into a useable format. Prototype regular expression parsing and user defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Cell Data Conversion A #####\n",
    "# Data conversion/parsing setup\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace, trim, col, lower, to_date, unix_timestamp, udf\n",
    "\n",
    "# extraction patterns\n",
    "re_us_currency = r'(\\d*\\.\\d\\d)'\n",
    "re_integer = r'(\\d*)'\n",
    "re_float = r'(\\d*\\.?\\d*)'\n",
    "\n",
    "\n",
    "# Example user defined function to customize parsing\n",
    "def Parse_Note(aStr):\n",
    "    return \"Noted: \" + aStr\n",
    "\n",
    "uMakeNote = udf(lambda x: Parse_Note(x),StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lineNum: integer (nullable = true)\n",
      " |-- aDate: date (nullable = true)\n",
      " |-- anInt: integer (nullable = true)\n",
      " |-- aFloat: float (nullable = true)\n",
      " |-- currency: decimal(10,2) (nullable = true)\n",
      " |-- aNote: string (nullable = true)\n",
      "\n",
      "+-------+----------+-----+------+--------+----------------+\n",
      "|lineNum|     aDate|anInt|aFloat|currency|           aNote|\n",
      "+-------+----------+-----+------+--------+----------------+\n",
      "|      1|2016-01-01|    1|   1.1|    1.10|Noted: good data|\n",
      "|      2|2016-01-02|    2|   2.2|    2.20|Noted: good data|\n",
      "|      3|2016-01-03|    3|   3.3|    3.30|Noted: good data|\n",
      "|      4|2016-01-04|    4|   4.4|    4.40|Noted: good data|\n",
      "+-------+----------+-----+------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Cell Data Conversion B #####\n",
    "\n",
    "# Dates can be problematic\n",
    "# to_date(regexp_replace(raw_data2.aDate,r'/',r'-')).alias('aDate')\n",
    "#  -above works if date is in format yyyy/mm/dd\n",
    "# unixTime soln from http://stackoverflow.com/questions/36948012/how-to-change-the-column-type-from-string-to-date-in-dataframes\n",
    "\n",
    "# Transform into new dataset/dataframe with appropriate data types\n",
    "parsed_data2 = (raw_data2\n",
    "                .select(regexp_extract(raw_data2.lineNum,re_integer,1).alias('lineNum').cast(\"int\"),\n",
    "                        to_date(unix_timestamp(raw_data2.aDate, \"yyyy/mm/dd\").cast(\"timestamp\")).alias('aDate'),\n",
    "                        regexp_extract(raw_data2.anInt,re_integer,1).alias('anInt').cast(\"int\"),\n",
    "                        regexp_extract(raw_data2.aFloat,re_float,1).alias('aFloat').cast(\"float\"),\n",
    "                        regexp_extract(raw_data2.aDollarAmount,re_us_currency,1).alias(\"currency\").cast(\"decimal(10,2)\"),\n",
    "                        uMakeNote(raw_data2.aNote).alias('aNote')\n",
    "                  )\n",
    "              )\n",
    "parsed_data2.printSchema()\n",
    "parsed_data2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Cell Data Conversion C #####\n",
    "# validation check: there should be no null values\n",
    "# Stay tuned!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Access File with Imperfect Data\n",
    "Read & parse a contrived sample of imperfect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lineNum: string (nullable = true)\n",
      " |-- aStr: string (nullable = true)\n",
      " |-- aDate: string (nullable = true)\n",
      " |-- anInt: string (nullable = true)\n",
      " |-- aFloat: string (nullable = true)\n",
      " |-- aDollarAmount: string (nullable = true)\n",
      " |-- aNote: string (nullable = true)\n",
      "\n",
      "+-------+------+---------+-----+------+-------------+----------------------------+\n",
      "|lineNum|aStr  |aDate    |anInt|aFloat|aDollarAmount|aNote                       |\n",
      "+-------+------+---------+-----+------+-------------+----------------------------+\n",
      "|1      |smart |1/1/2016 |1    |1.1   | $1.10       |good data                   |\n",
      "|2      |cows  |1/2/2016 |2    |2a    | $2.20       |float with text             |\n",
      "|3      |moo   |1/3/2016 |3.3  |3.3   | $3.30       |int as float                |\n",
      "|4      |longer|14/4/2016|4    |4.4   | $4.40       |bad date mo=14              |\n",
      "|5      |than  |8/15/2016|5    |5.5   |abc          |text in currency            |\n",
      "|6      |dogs  |9/12/2016|-6   |6.6   | $(99,999.00)|possible out of range values|\n",
      "+-------+------+---------+-----+------+-------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL Imperfect Data A #####\n",
    "# Read file from HDFS\n",
    "\n",
    "datafile_HDFS = r'hdfs://172.18.0.2:9000/user/root/testData/simpleSampleNotClean.csv'\n",
    "\n",
    "raw_data = spark.read.csv(datafile_HDFS, header = \"true\")\n",
    "raw_data.printSchema()\n",
    "raw_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lineNum: integer (nullable = true)\n",
      " |-- aDate: date (nullable = true)\n",
      " |-- anInt: integer (nullable = true)\n",
      " |-- aFloat: float (nullable = true)\n",
      " |-- currency: decimal(10,2) (nullable = true)\n",
      " |-- aNote: string (nullable = true)\n",
      "\n",
      "+-------+----------+-----+------+--------+-----------------------------------+\n",
      "|lineNum|aDate     |anInt|aFloat|currency|aNote                              |\n",
      "+-------+----------+-----+------+--------+-----------------------------------+\n",
      "|1      |2016-01-01|1    |1.1   |1.10    |Noted: good data                   |\n",
      "|2      |2016-01-02|2    |2.0   |2.20    |Noted: float with text             |\n",
      "|3      |2016-01-03|3    |3.3   |3.30    |Noted: int as float                |\n",
      "|4      |2016-01-04|4    |4.4   |4.40    |Noted: bad date mo=14              |\n",
      "|5      |2016-01-15|5    |5.5   |null    |Noted: text in currency            |\n",
      "|6      |2016-01-12|null |6.6   |999.00  |Noted: possible out of range values|\n",
      "+-------+----------+-----+------+--------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL Imperfect Data B #####\n",
    "parsed_data = (raw_data\n",
    "                .select(regexp_extract(raw_data.lineNum,re_integer,1).alias('lineNum').cast(\"int\"),\n",
    "                        to_date(unix_timestamp(raw_data.aDate, \"mm/dd/yyyy\").cast(\"timestamp\")).alias('aDate'),\n",
    "                        regexp_extract(raw_data.anInt,re_integer,1).alias('anInt').cast(\"int\"),\n",
    "                        regexp_extract(raw_data.aFloat,re_float,1).alias('aFloat').cast(\"float\"),\n",
    "                        regexp_extract(raw_data.aDollarAmount,re_us_currency,1).alias(\"currency\").cast(\"decimal(10,2)\"),\n",
    "                        uMakeNote(raw_data.aNote).alias('aNote')\n",
    "                  )\n",
    "              )\n",
    "parsed_data.printSchema()\n",
    "parsed_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note above the regular expression parse, as designed, pulled the best match rather than report an error. These patterns could be improved. The date with month=14 was parsed as January; probably not desirable behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 rows with null values: \n",
      "+-------+----------+-----+------+--------+--------------------+\n",
      "|lineNum|     aDate|anInt|aFloat|currency|               aNote|\n",
      "+-------+----------+-----+------+--------+--------------------+\n",
      "|      5|2016-01-15|    5|   5.5|    null|Noted: text in cu...|\n",
      "|      6|2016-01-12| null|   6.6|  999.00|Noted: possible o...|\n",
      "+-------+----------+-----+------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### CELL Imperfect Data C #####\n",
    "# programmatically check for null values.\n",
    "# We are assuming, of course, that null values are not permissible\n",
    "\n",
    "bad_rows_df = parsed_data.filter(parsed_data.lineNum.isNull() |\n",
    "                                 parsed_data.aDate.isNull() |\n",
    "                                 parsed_data.anInt.isNull() |\n",
    "                                 parsed_data.aFloat.isNull() |\n",
    "                                 parsed_data.currency.isNull() |\n",
    "                                 parsed_data.aNote.isNull()\n",
    "                                 )\n",
    "\n",
    "print \"There are {} rows with null values: \".format(bad_rows_df.count())\n",
    "bad_rows_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
